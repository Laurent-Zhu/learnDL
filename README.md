# learnDL
虽然之前已经接触过了一些深度学习的东西，但光有理论，动手实践太少，决定跟着《动手学深度学习：Pytorch版》再动手把代码敲一遍，以此仓库作为记录

- [x] 第二章主要是一些张量的操作、线性代数、概率论、微积分的一些预备知识
  - [x] 很重要的一点就是梯度和反向传播
  - [x] 在前向传播的时候可以设置require_grad=True来保存计算图，以便反向传播的时候计算各个参数的梯度，计算出梯度之后使用SGD、Adam等优化算法对参数进行更新
- [x] 第三章介绍了线性神经网络，包括最简单的线性回归以及Softmax回归
  - [x] 线性回归，常采用均方误差MSELoss作为损失函数
  - [x] Softmax回归处理分类问题，使用交叉熵损失作为损失函数
- [x] 第四章介绍了多层感知机，通过引入激活函数引入了非线性因素，还讨论了过拟合、欠拟合及其解决方法，最后还有一个kaggle房价预测的实战
  - [x] 模型复杂度较高（参数较多或者参数取值范围较大）但是数据集小，或者特征数量多而数据集小，容易导致**过拟合**，可以理解为背答案
  - [x] **权重衰减**作为最广泛的正则化技术之一，可以缓解过拟合，权重衰减的核心是在损失函数上面加上权重的L2范数（或者其他），使L2范数尽可能小来降低模型复杂度，进而避免过拟合
  - [x] **暂退法**是以p的概率丢弃隐藏层单元，只需要在每个全连接层之后添加一个暂退层，可以提高模型的平滑性（函数不应该对其输入的微小变化敏感），进而缓解过拟合
